An Architecture for Social Hierarchical Learning
================================================

We propose to construct an integrated architecture that accomplishes hierarchical learning under socially cooperative tasks between a robot and one or more human co-worker/instructors.  Our architecture will be implemented in three phases. These phases indicate levels of competency and task decomposition, but the development of this system will not necessarily proceed linearly from one phase to the next; the implementation of these phases will occur in an iterative fashion such that multiple phases will be under development simultaneously.  The first phase leverages reinforcement learning to acquire basic skill competency. The second phase involves learning the nature of the overall task presented to the system and decomposing it into sequences of subtasks. The third phase involves learning how to optimally assign roles in real-time, adapting SHL agents to collaborate with human co- workers to improve efficiency and performance. 

The architecture presented here will be implemented on one or more of the humanoid robots currently owned by our research group (see Figure 1 and Section 8).  Both of these robots have human-like body structure and human-like ranges of motion in the upper torso.  To maintain the focus of this experiment on the construction of collaborative learning and operation techniques, appropriate tasks will be chosen that match the physical manipulative capabilities of the robots.  Sensing will be augmented with external sensor packages for body tracking (including both an Ubisense whole-body tracker and a Vicon articulated-motion tracker, see section 8).   As will be demonstrated below, SHL will be designed to explicitly handle situations in which there are tasks which are more appropriately handled (or can only be handled) by the human co-worker or the robot, thus the selection of simplified manipulation tasks will not hinder the development of this system.  

Finally, to clarify the explanations below and to provide a concrete example of SHL operation, we will examine the task of a human and a robot collaboratively constructing a birdhouse.
  

Sub-skill Acquisition via Reinforcement Learning
---------------------------------------------------------
In the first phase of SHL, reinforcement learning is used to acquire component sub-skills. A human operator interacting with the system must first teach the robot the basic skills that it lacks. Due to the portability of skills from task to task, let us suppose that the robot already knows how to manipulate the raw materials required, but does not yet possess the knowledge required to manipulate a screwdriver. The operator may then teach the robot to use the screwdriver, first by showing how to hold it, then by applying torque to a screw. The robot should immediately be able to reproduce a similar action, but may not have an entirely accurate representation of the concept. Mistakes can be negatively reinforced by feedback from the human partner, eventually eliminating bad behavior and resulting in a new proficiency. Choosing not to train the robot on such primitives limits the eligible roles for it in the final phase.

Our phase 1 SHL model will use techniques like shaping to make full use of human guidance.  Shaping, or iteratively carving away and refining portions of state space, is used to quickly achieve desirable policies [z14, z32]. As opposed to building a near-optimal policy from an initially empty set of 'good' actions, solving the reverse problem of eliminating bad branches allows for rapid convergence to a passable policy, rather than necessarily achieving optimal performance. Real-time human feedback allows the system to avoid the pitfalls of any introduced uncertainty, allowing the agent to maintain its exploration policy when acting out of its explicit training mode. By leveraging the instructor's foresight to prevent the agent from traversing bad paths through the problem space (ceasing exploration in such directions as soon as it becomes evident), the agents can continue to improve as they perform the tasks more frequently. 

The learning task represented by phase 1 is similar to the current state-of-the-art in hierarchical learning systems and reinforcement learning systems [e.g, z15, z16, z17, z31].  The work in this phase will not advance the state-of-the-art significantly beyond these extant systems, but it is a necessary step for phases 2 and 3.  Furthermore, the infrastructure constructed during this phase will provide a set of basic skills which can be a basis for action segmentation (a task that this project does not address directly).     

If a human instructor were attempting to teach the robot how to assemble a birdhouse, phase one would consist of focused instruction to teach the robots simple component skills such as grasping a screwdriver, applying torque to a screw, or affixing one wall to another at a right angle.  These basic tasks should be components that are likely to see re-use either within the same task or across additional tasks.  The task of deciding which skills the robot should be taught, as well as the granularity of these basic skills, is left to the human instructor to decide.  This reflects a basic design principle of social hierarchical learning – that practical human-robot interaction systems should let robots do what robots do well and humans do what humans do well.  This allows us to consider situations in which (1) the human and robot have different skill sets, which may or may not overlap, or (2) that some sub-tasks might be preferred by either partner, or (3) that some tasks may be performed by one partner at sufficiently lower cost (in terms of time, resources, or other considerations).

Learning Social Task Structure
------------------------------
The second phase involves learning the structure of the task to be solved. Utilizing methods akin to learning from demonstration [z18, z19, z20, z21], the system will learn to sequence primitive actions to achieve a complex hierarchical task representation. The primary focus of this research is discovering, manipulating, and optimizing the collaborative structure of the task, rather than goal state discovery or primitive action learning. Research within robotics as it pertains to learning from demonstration typically involves a robot mimicking an action undertaken by a single human or robot. Within the context of shaping, learning from demonstration provides a reliable and rich heuristic to assist in the determination of useful states and paths through state space. Modeling an entire interaction, inclusive of all actors from initiation to goal state, is helpful but not altogether necessary for a SHL system to succeed. A robot placed in a situation where it was fully trained on the entire interaction would determine its role quicker and react more effectively to those it is working with. The same robot placed in a situation on which it is not fully trained could accomplish this learning through its own experience by observing the humans it is working with. 

The result of the second phase is a skill tree and sample traversal, with social metadata indicating potential roles that may be assigned within the skill tree traversal. This social metadata adds context to learned, portable skills and helps restrict the action search space.  While a substantial part of the proposed research will be to establish a representation for this skill tree that contains all of the required metadata for constructive collaboration, we provide below an example of the kind of data that we expect to be contained in this structure.  The hierarchical structure of the task must be learned simultaneously with the relevant social metadata, which also presents a substantial research topic for this project.  
 
After a single observation of a complex skill, the SHL system has constructed a hierarchy in which the complete task is divided into three sub-tasks (G, E, and H) which were observed to occur in a linear sequence (marked “G→E→H” in the figure to indicate G, followed by E, followed by H). Some of these sub-tasks were further divided into tasks that the system was able to recognize as belonging to its basic-level skill set (for example, that sub-task D is composed of two basic skills, e and b) while other sub-tasks are not recognized by the system (for example, the unknown task A).  Even after only a single observation in which the human instructor demonstrates all parts of the task, the system will conditionally label some tasks based on which agents can perform the task.  When the robot recognizes sub-tasks that it had learned under phase 1, these leaf nodes are labeled as potential tasks for either the robot or the human partner (green boxes in the figure).  Any higher-level task composed of robot-feasible sub-tasks will also be similarly labeled.  

After three observations of the skill (Figure 2, middle), the SHL system has refined the skill tree for this task based on these two additional observations.  In this example, four refinements have been made to the skill tree.  First, the system has learned that tasks G and E can be done in parallel but must both be performed before task H (denoted as “(G||E)→H” in the root node of the figure).   This would result from an understanding both that both sequences GEH and EGH had been observed and that there were no dependencies between tasks G and E.  Second, the system has learned that task F is actually another occurrence of primitive action a.  This refinement is likely to be commonplace as we assume that action recognition is noisy.  Third, task A has been changed from a human-only task to a mixed-agent task.  This update occurs as a result of either the recognition of a robot-required step as part of  the decomposition of A or that a robot can participate in some of the action, but it does not have a high enough confidence in its segmentation to participate. Finally, the robot has observed that the two components of skill C can be completed in parallel.  These incremental changes to the skill tree continue as more and more instances of task execution are observed.

After five observations of the skill (Figure 2, bottom), the SHL system has completed a fully-specified tree, that is, each leaf of the tree consists of an indentified basic-level action which need not be decomposed further.  Further refinements are still possible, especially concerning the sequencing of tasks, but this stage is sufficient for the robot to engage collaboratively in all aspects of the sample task.  Four principle changes to the skill tree have occurred between the previous figure and this one.  First, task A has been determined with high confidence to be a sequence of primitives. One of these primitives (d) is a task that is best performed by the robot or can only be performed by the robot.  (Observations of this type can occur even for human-demonstrated tasks if the robot is tele-operated for portions of the task or if the task is specified by the instructor as being a robot-specific task.)  Second, task G has been updated to indicate that its components may be performed in parallel and re-colored to indicate that completion of this task requires both a human partner (because of the presence of a human-only sub-task a) and a robot partner (because of the robot-only sub-task d).  Third, task E has been encapsulated as a single primitive human-only action (g).  Further observations might cause a decomposition of this primitive skill, but it is sufficient based on the initial observations to leave that task to the human partner.  Finally, the root node has been updated to indicate that both a human partner and a robot partner are required to complete the task as a whole. 

While these examples do not completely specify an algorithm for the construction of these hierarchical skill trees, a primary component of the proposed research is to design and implement this algorithm.  The examples provide only a sketch of what this process must involve.

Dynamic Role Assignment and Task Execution
------------------------------------------
The final phase involves learning to produce role assignments dynamically in an effective and safe way. Reinforcement learning is applied to teach the system how to optimally divide labor amongst autonomous workers in response to the human workers' actions. This feedback regarding the system's assignment and parallelization of tasks to agents guides the autonomous agents in better allocating their resources. The primary challenge of this phase is analyzing the social metadata and applying it to both the skill tree and generated action sequence ("solution"). The result of this process is a role assignment tree, a flexible assignment structure that compensates for unknowns and uncontrollable agents in the overall task completion plan. 

For example, in constructing a birdhouse, the robot does not know if the human will choose to construct the four walls of the base or assemble the roof first. The system has previously observed that these tasks can occur in parallel. If the human chooses to assemble the walls, the robot must choose between assisting with the construction of the walls and beginning the roof assembly on its own.  Regardless of which task the robot begins to perform, the robot must continually monitor the progress of the human worker so that it can provide assistance if difficulties are encountered by the human worker.  Supposing the human has partially performed the subtask of joining two walls but has not yet screwed them together, the robot may notice that both of the human's hands are occupied and must intervene to continue making progress. The robot will then have the option to apply the screws to join the walls before continuing its currently assigned task. This type of reactive, collaborative system supports side-by-side interactions that are flexible enough to conform to the individual differences between a group of possible collaborators that result from either personal preferences or differences in human skill levels with particular aspects of the task.  

             

Benefits of Social Hierarchical Learning
========================================

Flexible, dynamic cooperation on this level has never been attempted and would greatly benefit hierarchical learning systems. Real-time, reactive role determination and learned task execution combine to form a powerful system enabling side-by-side collaboration between humans and robots. Roles may inherently carry restrictions within this flexible role assignment paradigm, as it can be learned that certain tasks may be better suited for robots to complete and some tasks should exclusively be performed by humans. Reinforcing its role assignments over time, the system provides net improvement with each successive task completion.  The timing for Social Hierarchical Learning is ideal, as the many necessary innovations required for its development have only very recently progressed to a point where it has become a feasible distance from the current state of the art.

