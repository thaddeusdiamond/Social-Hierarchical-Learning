% File: report.tex

\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\pdfoutput=1
\pdfinfo{
/Title (Primitive Recognition using Inverse Q-Table Matching)
/Subject (In Proceedings AAAI)
/Author (Bradley Hayes, Brian Scassellati, and Thaddeus Diamond)
}
  
\begin{document}
\title{Scene Decomposition using Inverse Q-Table Matching}
\author{Bradley Hayes, Brian Scassellati, and Thaddeus Diamond\\
Yale University\\
Arthur K. Watson Hall\\
51 Prospect St\\
New Haven, Connecticut 06511\\
}

\maketitle

\begin{abstract}
Several variations of Q-Learning have reliably demonstrated that reinforcement
learning is a viable technique for learning skills in robotic systems.  However,
very little work has focused on using sensory data to generate an approximation
of which Q-Table most reliably approximates the skill just observed.  We present
herein a system which is able to consistently provide accurate decompositions of
a scene into simple, learned primitives by using what we are terming
\textit{Inverse Q-Learning}.  The system is then able to take this
frame-by-frame decomposition to provide an approximation for a suitable 
hierarchical structure of the task being observed.  This structure is composed
entirely of either previously learned primitives or unknowns.  Once those
learned actions have been successfully mapped over time, the entire scene, or
merely portions of it, can be replicated by robotic agents in a mixed
human-robot environment.  
\end{abstract}

\section{Introduction}
\label{sec:intro}
Empirical data has consistently proven that reinforcement learning can be used
as a viable method to learn primitive skills in a real robotic systems.
Standard Q-Learning techniques which traditionally take several hundred or
several thousand trials to converge on a learned skill have been supplanted
by techniques yielding convergent results within some small margin of error,
which require orders of magnitude fewer trials.   Although there are too many
to enumerate here, some of the techniques include the use of simple external
feedback such as "good" and "bad" indicators \cite{TAMER} or an animal
clicker \cite{Clicker}, the use of intermittent human
guidance \cite{AdviceTaking,TeacherRL}, and the integration of human
demonstration \cite{DemonstrationRL}.  These results
have made compelling the use of modified Q-Learning in real robotic systems.

Very little work has focused on using data gathered during the Q-Learning
process to later decompose a complex scene into a set of primitives via
observation.  The concept of "inverse reinforcement learning" is not entirely
novel, as Abbeel and Ng \cite{InverseRL} used inverse reinforcement learning
to generate reasonable approximations for the reward function underlying an
observed primitive.  However, their work does not leverage existing data
to create approximate classifications, but rather uses raw sensory data to
attempt to recover the unknown reward function.

We present herein a system that, using Q-Tables generated from simple
Q-Learning in a real robotic system, uses observation to 
It is important to separate the context of our contribution from that of
existing research in \textit{learning from demonstration} (LfD).  Standard
techniques in reinforcement learning have focused on using examples to
transfer a policy representing some basic skill from a teacher to a
pupil \cite{JenkinsLFD,LFDSurvey}.  We focus on the use of
\textit{existing knowledge} to generate an approximate decomposition of
complex scene data.

\section{Background}
\label{sec:background}
\subsection{American Sign Language Gesture Recognition}
In this section we discuss relevant background to the problem.

\subsection{Hierarchical Task Decomposition}

\section{Primitive Recognition}
\label{sec:recognition}
In this section we discuss our work on primitive recognition.

\section{Results}
\label{sec:result}
In this section we discuss our experimental results.

\section{Related and Future Work}
\label{sec:future}
In this section we discuss future work (hint at SHL?)

\section{Conclusions}
\label{sec:conclusions}
In this section we conclude.

\section{Acknowledgements}
\label{sec:acknowledgements}
We would like to thank... grant numbers?  Brad's grad school funding?  God?
Wayne Brady?

\bibliography{report}
\bibliographystyle{aaai}

\end{document}attempt 
