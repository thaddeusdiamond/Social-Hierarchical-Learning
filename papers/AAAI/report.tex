% File: report.tex

\documentclass[letterpaper]{article}
\pdfoutput=1
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[pdftex]{graphicx}
\usepackage{amssymb}
\frenchspacing
\pdfinfo{
/Title (Using A Hierarchical Architecture for Collaborative Social Tasks to Reinforce and Recognize Action Primitives)
/Subject (In Proceedings AAAI)
/Author (Bradley Hayes, Thaddeus Diamond, and Brian Scassellati)
}
  
\begin{document}
\title{Using A Hierarchical Architecture for Collaborative Social Tasks to Reinforce and Recognize Action Primitives}
\author{Bradley Hayes, Thaddeus Diamond, and Brian Scassellati\\
Yale University\\
Arthur K. Watson Hall\\
51 Prospect St\\
New Haven, Connecticut 06511\\
}

\maketitle

\begin{abstract}
Primitive skill acquisition for robots has been accomplished through a variety
of techniques, many utilizing Q-Learning. Recent work has shown that with human
assistance, complex actions can be learned with limited training data. The
resulting skill representations can be leveraged to replay flexible behaviors
that dramatically increase the utility of the robot. Despite this, existing
systems have yet to address the greater problem of cooperative action execution.
For a robot to effectively cooperate with either human or robotic coworkers,
it is necessary that the robot be capable of modeling and recognizing its
coworkerâ€™s behaviors within its environment. We approach the task of
understanding and predicting coworker actions by providing a novel, fast algorithm
allowing a robot to solve the inverse problem of skill execution, skill
recognition, in real-time. We demonstrate our results through the recognition of selected
American Sign Language gestures. Our work is unique in that it extends a popular existing skill
representation paradigm to a unified data structure capable of both skill execution and skill recognition. 
Our algorithm provides a coded timeline of
primitive actions generated in real-time, correcting its hypotheses as more
information is presented. This data can then be directly used in a multitude
of ways, from within a planning system for the coordination of a robot's
actions to building a skill hierarchy fully decomposing a complex cooperative 
task into primitive actions.
\end{abstract}

\section{Introduction}
\label{sec:intro}
% 1. Talk about the need for cooperative robotics here.
	As research in recent years has demonstrated, robots operating in a collaborative environment (co-robots) need to be flexible agents, adapting to the changing needs of their daily operations. An excellent example of such a system is Robonaut, a humanoid robot designed to match the dexterity of a suited astronaut \cite{Robonaut}. This platform is designed for both autonomous collaboration and teleoperative capabilities, a desirable set of operating modes for assisting human co-workers in the particularly challenging environments afforded by space exploration. 
	
  The introduction of such systems requires a substantial effort not merely aimed at producing capable and robust robots, but also a careful and planned interaction methodology allowing non-expert human operators to contribute to the robot's training with a non-invasive, intuitive procedure.  Observation has been used throughout the entirety of human history as a method for teaching colleagues skills required in a collaborative effort.  Therefore, it is greatly important that a co-robot not only achieve competency through direct skill training, but also that it utilize observations of others in its everyday behaviors to reinforce and improve existing abilities.

  For systems embodying these characteristics, simply maintaining an awareness of peer actions is critical for safety, efficiency, and general performance. However, direct skill training is still, of course, a vital part of a co-robot's success.  Knox, et. al. continue to make progress towards allowing non-experts to guide skill acquisition in such systems, facilitating the design of agents with shapable behaviors \cite{TAMER2}. We propose a method that robotic agents can leverage to help achieve this awareness while simultaneously using the classified data to improve its own performance and understanding of low level actions.  Our methodology relies on the use of a simple Q-Learning variant to model skills as Semi-Markov Decision Processes (SMDPs).  However, we also present a compatible method for utilizing a generic hierarchical skill architecture to achieve improvement in SMDP-based skill execution while simultaneously providing the ability to recognize known skills when being performed by other agents. 

Our system is demonstrated by utilizing the observation and subsequent automated identification of selected American Sign Language gestures to improve an agent's understanding of the underlying primitive skills involved. We then present the significance of this contribution, grounded within the rapidly expanding field of collaborative robotics.

\section{Background and Related Work}
\label{sec:background}
\subsection{Skill Recognition}
%1. Talk about LBD as an accessible means for non-experts to train robots on skills.
Several methods have been proposed for automating skill acquisition in robots.  However, many of these have relied on using expert knowledge to transfer knowledge of a specific domain to a system through programming.  Learning by demonstration (LBD), by contrast, is a technique that relies on pre-recorded samples as a means for experts and non-experts alike to train robots on primitive actions.  The automation and accessibility of such methods have increased the complexity and variety of skills that a robotic system can successfully acquire.

%2. Introduce concept of learning skills through traditional robot QLearner system, talk about result: qtable and policy indicating possible executions of skill.
How such a robotic system might represent primitive actions internally can vary.  However, traditional robotic systems often use Markov Decision Processes in combination with Q-Learning to accomplish this goal.  The result of such an effort is both a \textit{Q-Table}, representing the known states in a given environment and the corresponding rewards for transitions between them, and a policy indicating all possible executions of a skill.  Such a policy, in the simplest case, can be represented as a greedy traversal of the MDP until the goal state is reached.

%3. Introduce idea that this information can also be leveraged to identify others' execution of these skills
Because a Q-Table can be viewed as an arbitrary n-dimensional representation of a given state space, it can easily be leveraged to identify the execution of known policies in others.  For example, assume a state space represents the positions of a right hand, right elbow, and right shoulder in three-dimensions.  A robotic system with a pre-defined dictionary of gestures could use standard statistical methods to decompose sensor data into timeframes labeled according to which of the robot's policies most closely matched during that interval.  In fact, that is exactly what our system does.

%5. Generalize result beyond gesture recognition to other motor tasks, cite konidaris' work
It is important to note that gesture recognition is not the only domain in which our system is viable.  Several common motor tasks, such as tool manipulation and collaborative problem solving requiring an external perspective \cite{HRITraftonPerspective} are well-suited to utilizing a previously acquired set of action policies.

% 6. Present work as an enhancement layer for konidaris' CST work, generalizing it to automatically work for multiple skills, properly applying training data.
The notion of using skill transfer is not entirely novel.  Konidaris et al. have successfully leveraged existing learned skills to perform manipulation of simple tools \cite{AutoSkillAcquisition}. However, our major contribution is that the skill transfer resides not in transferring knowledge across domains, but rather enhancing the utility of training data through reuse and reinforcement during object recognition. Moreover, because we utilize primitive actions learned independently to an observation stage, we can generalize the observed samples to include arbitrarily many individually executed skills that can be identified by spawning several ``recognizer threads'' at once.


\subsection{Hierarchical Skill Architectures}
% 1. Talk about the need for hierarchical representation when multiple skills are required
Within the context of robotics, two areas of reinforcement learning research that are particularly relevant to co-robot advancement are autonomously building options hierarchies and reducing the trials or information required to achieve new proficiencies. The former heavily overlaps with non-robot-centric reinforcement learning work, which aims to make high-dimensional, continuous domain problem spaces tractable. General solutions are difficult to formulate, due to the vastness of potential methods and goals for such a system. Hierarchical learning systems follow the philosophy that these highly complex problems can very likely be broken into a number of simpler, tractable problems. Discovering the most efficient way to discover and arrange the low-level primitive actions is an open problem, with a multitude of viable approaches \cite{EfficientSkillLearning,AutoHierarchyLearning,LearningHierarchicalControl}.

% 2. Multiple skills makes the agent more adaptable and useful
Konidaris, et. al. have successfully shown that the latter can be influenced through the former, exploiting hierarchical learning to enable skill transfer between environments \cite{AutoSkillAcquisition}. By extending the usefulness of a skill laboriously acquired through trial and error, the ratio of the cost of the learned action to its overall utility is reduced substantially. Achieving a general solution for attaining the cognitive capability to recognize instances where already-known actions can be transferred and applied would constitute a significant advance for collaborative robotics. This is emphasized through the ever-increasing incorporation of demonstration-as-training-signal that is becoming more prevalent as the computational costs for doing so diminish. As this incorporation continues, barriers to dynamic, adapatable multi-agent, multi-human collaboration will be removed. 
	
% 3. Talk about decomposing skill sequences into Task plans
A hierarchical skill architecture can be used to great effect when assisting in collaborative task execution. To maximally benefit the collaborative aspects of complex task execution, it is essential for all agents involved to have reasonable approximations of the intent and plan of each other peer worker. With humans being naturally receptive of skill scaffolding, expressing a complicated task in terms of actions, sub-actions, and temporal sequencing, the intentions of each agent can be effectively communicated in a common format. One of the greatest technical challenges faced when orchestrating a collaborative exercise between multiple agents including both humans and robots is the mutual recognition of intention and action. Were there a method by which a participant or external observer could automate this process, a significant hurdle to this capability would be cleared.
 
% 5. Accounting for multiple skills complicates the learning and execution problem
Work by Martinson, et. al. has shown the benefits conferred to an agent utilizing combinations of pre-tested behavioral assemblages to successfully accomplish a complex goal in a dynamic environment outside the agent's control \cite{QLBehaviorSelection}. The paper associates a list of sensor states, referenced as perceptual triggers, with constructed sets of pre-trained actions with stable policies. This decision serves to reduce the MDP search space when considering potential skills to apply, while enhancing the insight human operators have into the agent's potential actions by providing an intuitive mechanism by which action/reaction pairs can be specified. Mugan and Kuipers approached this problem similarly to Konidaris, in that they look to autonomously abstract portable options that can be transferred between environments from their agent's experiences. Particularly of note, the QLAP algorithm operates through qualitative representations of the continuous world. This results in providing a tolerance for incomplete input information while disregarding irrelevant dimensions of the state space. As such, QLAP learns action hierarchies while maintaining both temporal and state abstraction \cite{AutoHierarchyLearning}. From a practical standpoint, maintaining state abstraction is crucial for maintaining usable performance levels as most real-world examples cannot be directly approached due to intractably large state space.

\subsection{Q-Learning in Robotics}
% 2. Talk about Q-Learning applied to robotics and skill acquisition here.
Markov Decision Processes provide a convenient and efficient mechanism to create flexible, arbitrarily complex options: closed-loop policies describing action sequences \cite{SuttonMDP}. The temporal abstraction and generality of representation make this a favorable method of internally representing knowledge about actionable skills. When designing for a non-expert audience, it is often favorable to trade complexity (and, on occasion, optimal resource allocation) for simplicity. This accessibility contributes to Q-Learning being one of the most widely utilized reinforcement learning methods within robotics \cite{QLearningWatkins}. Utilizing an environmental reward function, solving for an optimal action policy can often be accomplished autonomously and is only limited by the complexity of the state representation.

% 3. Talk about recent works that reduce the number of trials required to learn a skill.
Due to the exploration space of the generic QLearning problem scaling exponentially with dimensionality, heuristics are actively being researched that reduce the amount of trials required to achieve desirable policies. One example heuristic includes utilizing human feedback as both an immediate and anticipatory reward signal, leveraging human foresight to effectively reduce the search depth required to learn acceptable paths through state space \cite{TAMER}. Another effective heuristic involves leveraging data obtained through observing demonstrations of skills to favorably influence transition probabilities within the MDP, achieving accelerated convergence to an acceptable policy \cite{LFDSurvey}. Works like these reduce the amount of trials required by several orders of magnitude with minimal sacrifice in resulting skill quality.

% 4. Talk about high level contribution to solving this problem.
For a co-robot to interact as a productive member of a team, it must be able to recognize and correctly classify peer behaviors. This situational awareness is instrumental in achieving intelligent cooperation, of the same or greater importance as being capable of executing actions independently. To achieve this on a practical level, it is necessary to intelligently group sets of primitive actions together into complex options. 

\section{Definition of Problem Space}
\label{sec:pspace}
%1. Technically define the problem space being utilized, framed as the same data structure that can be leveraged for executing these actions. 
The agent and operating environment are represented by a continuous-state, continuous-time Semi-Markov decision process, defined by the tuple $(S,A,\mathbb{P},\Lambda,\gamma,D,R)$, where $S$ represents a continuous set of states, $A$ is a set of actions, $\mathbb{P}$ is a set of policies referred to as primitive actions, $\Lambda$ is a set of $\epsilon-$neighborhoods defined for readings from each input sensor comprising the state descriptor, $\gamma \in [0,1)$ as a discount factor, $D$ is the initial-state distribution, and $R : S \times A \rightarrow \mathbb{R}$ is an unknown state-action reward function. This data structure is representative of those commonly utilized within skill training and execution systems. We further define $S \rightarrow \mathbb{R}^k$ for $k$ sensor values.

We assume the agent holds limited a priori knowledge of a set of primitive skills $\mathbb{P}$, each describing a policy $P_\pi$ for traversing the agent-environment SMDP. Each primitive action contains its own exploration function, $P_e$, to guide its traversal while solving for known branches of the problem-space MDP. Additionally, each primitive $p \in P$ contains metadata indicating the estimated time required for completion of the primitive $P_t$. Completion of a primitive action is defined as traversing state space from an unknown initial state to with the $\epsilon-$neighborhood of a known goal state.

Each policy $P_\pi$ can be described as a series of decision rules to be applied when appearing in familiar situations. These decisions may range from complete deterministic to randomized, and are influenced by the exploration function $P_e$. Within each primitive, we introduce the notion of reward layers, able to be utilized to influence exploration transition probabilities but not artifically affect reward signal propagation.

Our work does not assume a uniform sensor refresh rate, as a state sampled at a given time may represent stale data from a subset of its input sensors. The algorithm presented has shown robustness to lossy sampling by remaining capable of confidently classifying observations under such conditions.

% opportunistic inverse reinforcement learning as applied to skill recognition. 
% Field robotics operates within a continuous-space Markov decision process

%2. Formalize SMDP being used, and each skill representation as a QLearner. Use Ng paper as a basis for this. Also include extra metadata being considered, such as time-to-execute and tolerance variables

%3. Define policy in terms of the exploration function being utilized, explain that this function can be swapped depending on the agent's goals.

% 4. Define linear combination of policies (reward layers)

% 5. Define the optimal policy in terms of the training data being input

% 6. Define MDP state vector as being a composite of disparate sensor data -- different refresh rates
% 7. Self-loops permitted but do not contribute reward


\section{Primitive Recognition Algorithm}
\label{sec:recognition}
1. Re-iterate problem: Given a sample demonstration, we seek to determine which known skill is being presented (if any),
                       if it should be considered an expert demonstration, and to use the new observation to simultaneously
                       improve recognition of the action as well as improve its execution potential.

2. Unlike solving the skill execution problem, the algorithm does not have explicit control over the policy being enacted over the state space.

3. Unlike the direct inverse reinforcement learning problem, there are multiple primitive candidates and the demonstration given cannot be assumed to be an example/training data for any of them.

4. Describe Algorithm:

	*At the specified sampling rate, poll all sensors to create a state descriptor.
	*Iterate through all eligible primitive actions:
		* If transition from previous state to this state is defined and has a positive reward, add to hit states
		  otherwise if it isn't and the sliding window, of size = antipicated duration * tolerance * sampling rate, contains less
		  than X percent hit states, we time-out the primitive as an optimization step, resetting its sliding window size to 0.
		* If the current state is within an e-neighborhood of an explicitly trained goal state (don't use intuited goal states,
		  otherwise the goal area will creep outwards):
			* For each hit state:
				* With probability p, designate current hit state as a waypoint. Add a waypoint policy that heavily favors
				  transitions to this state from all other eligible states.
			* Compute A, the percentage of hit states within the window.
			* Beginning with the first hit state, compute B: the expected mean transition reward from following the waypoint policy through to a goal state. Known as the "realistic path".
			* Beginning with the first hit state, compute C: the expected mean transition reward from following an optimal policy through to a goal state. Known as the "optimistic path".
			* Compute D, the duration elapsed when traversing the realistic path, assuming a consistent sampling rate
			* Compute E, the expected duration of an ideal example of skill execution
			* Compute F, the mean transition reward for following an optimal policy through the state space fitting the duration requirements.

			* Compute the confidence score: $0.25A + 0.4B/C + 0.1D/E + 0.25B/F$
			* Apply a <score,label> tuple to each frame of data within the sliding window.



5. Analyze Algorithm:

	* Guards against failure
	* Guards against exponential state/transition growth
	* Benefits to execution side of the action primitives being trained
	* What happens with misclassification -- robustness against slippery-slope failure mode
	* Execution time required per primitive? How does the time requirement scale as primitives are added?

\section{Experimental Setup}
\label{sec:experiment}
We chose to test our system on properly identifying and segmenting video of American Sign Language (ASL) gestures. American sign language provides a practical use case while testing the algorithm's tolerance to variance in training, as each training example will necessarily be unique. Our state space encompasses six dimensions, corresponding to the relative x, y, and z coordinates of each hand involved.  Our system has \textit{a priori} knowledge of a partially overlapping set of ASL gestures, having been giving limited training for each known gesture.

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{scatter_plot_left_hand.eps}
\caption{Overlapping Right Hand Positions in ASL Signs}
\label{fig:asl_sign_overlap}
\end{center}
\end{figure}

% BRAD: Thad moved this here because it seemed like a better place than intro
% 7. Present ASL recognition as an example of a complex skill set to identify, as inconsistency and noise are more likely prevalent with human repetitions of actions than precise robotic replay.
Our reasons for using ASL here are several.  First and foremost, ASL requires, at a minimum, a six-dimensional state space representation.  This provides sufficient complexity to prove that our skill representation can be used in environments where brute force and other na\"ive methods would be intractable.  Furthermore, the noise associated with human performance of the ASL symbols is large enough to ensure that our recognition techniques rely on the overall trajectory of the training data, and not merely frame-by-frame matching of exact states over time.  Finally, the ASL gestures chosen overlap significantly, as seen in figure \ref{fig:asl_sign_overlap}, avoiding segmentation based on region of the state space that a primitive action occupies.

Gesture recognition for ASL symbols is not novel.  Our attempt here is not to recognize gestures with an extremely high degree of fidelity.  Although an interesting problem in its own right, several researchers have already made formidable contributions in this arena \cite{HandGestures,HSMMRecognition,POMDPGesture,HoughASL,ASLRealTime,MotionASL}.  What is interesting here is using our system to perform frame-by-frame classification of which gesture is likely being performed, while simultaneously improving the accuracy of an internal policy representing that respective gesture.

\section{Results}
\label{sec:result}

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{output_nowaypoints_onetrial.eps}
\caption{Initial Recognition Without Waypointing}
\label{fig:output_nowaypoints_onetrial}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{output_nowaypoints_repeatedtrials.eps}
\caption{Repeated Recognition Over Time Without Waypointing}
\label{fig:output_nowaypoints_repeatedtrials}
\end{center}
\end{figure}

In this section we discuss our experimental results.

\section{Related and Future Work}
\label{sec:future}
It is important to differentiate our contribution from that of
existing research in learning from demonstration.  Much previous research
within reinforcement learning has focused on using examples to
transfer a policy representing some basic skill from teacher to
pupil \cite{JenkinsLFD,LFDSurvey}.  Our algorithm focuses on the use of
\textit{existing knowledge} to generate an approximate decomposition of
complex scene data while simultaneously using our classifications and 
observation to continue to shape actionable policies.

While work partially overlapping our goals has been published previously
\cite{LearningBehaviorFusion}, our work differs substantially in terms of both
our algorithm and the resulting simultaneous benefit to observation
classification and action execution. The problem of hierarchy assignment is
left for other methods to more fully explore.

Another issue that is left unexplored in this discussion is that of garbage collection.  As we discuss, a crucial part of the algorithm provided is to limit the window of comparison to the length of the recorded primitive plus some epsilon, enabling its feasibility for use as an online algorithm.  However, a separate measure of the ``leanness'' of the algorithm is the amount of memory it consumes.  Currently, every observed state is added to an action's representative MDP, and it is connected to all possible neighboring states via interpolation.  This could easily lead to a ``state space explosion,'' where the primitive skills each contained an enormous amount of unnecessary states in their MDP.  This problem could be solved cursorily by providing a garbage-collector thread that continuously removes states with transitions whose values are below some threshold $\tau$.

Furthermore, it is crucial that this system be deployed in a complex, collaborative, multi-human multi-agent environment.  For example, given a limited set of observation data from a factory floor, a robotic system using our algorithm should have been able to decompose the scene into several agents performing several primitives simultaneously.  Once deployed in the actual factory, the robot should be capable of both performing actions it has learned in cooperation with humans and continue gathering observational data in order to improve its understanding of the task construction and policy execution.


\section{Conclusions}
\label{sec:conclusions}
We have presented herein a method for action recognition that simultaneously leverages an existing representation of primitive skills and improves the accuracy with which those skills can be performed by the observer.  This real-time recognition of learned actions is essential for any co-robot to be able to cooperate effectively in a multi-human multi-robot environment.

Other works have offered solutions to many of the challenges standing in the way of mainstreaming collaborative robotics, including automating option scaffolding through demonstration \cite{AutoSkillAcquisition}, acquiring skills utilizing human reward signals in place of \cite{TAMER} or in addition to \cite{TeacherRL} a traditional objective function, and correcting for critical omissions in non-expert demonstrations \cite{PerspectiveTaking}. Simplifying the human trainer/robot student interaction paradigm by shaping human guidance into usable forms is also rapidly advancing \cite{TAMER,Clicker,AdviceTaking,TeacherRL,DemonstrationRL}. These successful results strongly contribute to the widespread use of Q-Learning variants in real robotic systems and, more generally, to the feasibility of real-time human-robot collaboration.

\section{Acknowledgements}
\label{sec:acknowledgements}
Get grant details from Scaz

\bibliographystyle{aaai}
\bibliography{report}

\end{document}
